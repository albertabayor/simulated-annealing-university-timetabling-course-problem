IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

72 1

Stochastic Relaxation, Gibbs Distributions, and
the Bayesian Restoration of Images
STUART GEMAN AND DONALD GEMAN

Abstract-We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of
edges are viewed as states of atoms or motecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also
determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are
the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or
additive noise, the posterior distribution is an MRF with a structure
akin to the image model. By the analogy, the posterior distribution defimes another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states ("annealing"), or
what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution
yields the maximum a posteriori (MAP) estimate of the image given the
degraded observations. The result is a highly parallel "relaxation" algorithm for MAP estimation. We establish convergence properties of the
algorithm and we experiment with some simple pictures, for which
good restorations are obtained at low signal-to-noise ratios.
Index Terms-Annealing, Gibbs distribution, image restoration, line
process, MAP estimate, Markov random field, relaxation, scene model-

ing, spatial degradation.

I. INTRODUCTION
T HE restoration of degraded images is a branch of digital
picture processing, closely related to image segmentation
and boundary finding, and extensively studied for its evident
practical importance as well as theoretical interest. An analysis of the major applications and procedures (model-based and
otherwise) through approximately 1980 may be found in
[47]. There are numerous existing models (see [341) and
algorithms and the field is currently very active. Here we
adopt a Bayesian approach, and introduce a "hierarchical,"
stochastic model for the original image, based on the Gibbs
distribution, and a new restoration algorithm, based on stochastic relaxation and annealing, for computing the maximum
a posteriori (MAP) estimate of the original image given the degraded image. This algorithm is highly parallel and exploits
the equivalence between Gibbs distributions and Markov random fields (MRF).
Manuscript received October 7, 1983; revised June 11, 1984. This
work was supported in part by ARO Contract DAAG-29-80-K-0006
and in part by the National Science Foundation under Grants MCS-8306507 and MCS-80-02940.
S. Geman is with the Division of Applied Mathematics, Brown University, Providence, RI 02912.
D. Geman is with the Department of Mathematics and Statistics, University of Massachusetts, Amherst, MA 01003.

The essence of our approach to restoration is a stochastic
relaxation algorithm which generates a sequence of images that
converges in an appropriate sense to the MAP estimate. This
sequence evolves by local (and potentially parallel) changes in
pixel gray levels and in locations and orientations of boundary
elements. Deterministic, iterative-improvement methods generate a sequence of images that monotonically increase the
posterior distribution (our "objective function"). In contrast,
stochastic relaxation permits changes that decrease the posterior distribution as well. These are made on a random basis,
the effect of which is to avoid convergence to local maxima.
This should not be confused with "probabilistic relaxation"
("relaxation labeling"), which is deterministic; see Section X.
The stochastic relaxation algorithm can be informally described as follows.
1) A local change is made in the image based upon the current values of pixels and boundary elements in the immediate
"neighborhood." This change is random, and is generated by
sampling from a local conditional probability distribution.
2) The local conditional distributions are dependent on a
global control parameter T called "temperature." At low temperatures the local conditional distributions concentrate on
states that increase the objective function, whereas at high
temperatures the distribution is essentially uniform. The limiting cases, T= 0 and T= oo, correspond respectively to greedy
algorithms (such as gradient ascent) and undirected (i.e.,
"purely random") changes. (High temperatures induce a loose
coupling between neighboring pixels and a chaotic appearance
to the image. At low temperatures the coupling is tighter and
the images appear more regular.)
3) Our image restorations avoid local maxima by beginning
at high temperatures where many of the stochastic changes
will actually decrease the objective function. As the relaxation
proceeds, temperature is gradually lowered and the process
behaves increasingly like iterative improvement. (This gradual
reduction of temperature simulates "annealing," a procedure
by which certain chemical systems can be driven to their low
energy, highly regular, states.)
Our "annealing theorem" prescribes a schedule for lowering
temperature which guarantees convergence to the global maxima of the posterior distribution. In practice, this schedule
may be too slow for application, and we use it only as a guide
in choosing the functional form of the temperature-time dependence. Readers familiar with Monte Carlo methods in statistical physics will recognize our stochastic relaxation algorithm as a "heat bath" version of the Metropolis algorithm
[421. The idea of introducing temperature and simulating an-

0162-8828/84/1100-0721 $01.00 © 1984 IEEE

IEEF IRANSACTL()NS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

72 2

Tiealing is due to Cern' [8] and Kirkpatrick et al. [401 , both
of whomn used it for combinatorial optimization, including the
traveling salesman problem. Kirkpatrick also applied it to
computei- design.
Since our approach is Bayesiani it is model-based, with the
"model" captured by the prior distribution. Our models are
"hierarchical," by which we mean layered processes reflecting
the type and degree of a priori knowledge about the class of
images under study In this paper, we regard the original
image as a pair X (F, IJ) where F is the matrix of observable
pixel intensities and JI denotes a (dual) matrix of unobservable
edge elemiients. Thus the usual gray levels are considered a
marginal process. We refer to F as the intensity process and L,
as the line process. In future work we shall expand this model
by adjoining other, mainly geometric, attribute processes.
The degradation model allows tor noise, blurring, and some
nonlinearities, and hence is characteristic of most photochemical and photoelectric systems. More specifically, the degraded
image '(,t is of the torm O(H(F))O 'N, where H is the blurring
matrix, 0 is a possibly nonlinear (memoryless) transformation,
,is an independent noise field, and (i denotes any suitably invertible operation, such as addition or multiplication. Surprisingly, these nonlinearities do not affect the computational
burden.
To pin tlhings down, let us briefly discuss the Markovian
nature of the intensity process; similar remarks apply to the
line process, the pair (F, 1i), and the distribution of (F, IL)
conditional on the "data" 0X. Of course, all of this will be discussed in detail in the main body of the paper.
Let Zn i{(i, j):1 I< i, f < tn} denote the m X m integer lattice; then F {Fi,;} (i, j) e Zm, denotes the gray levels of
the original, digitized image. Lowercase letters will denote the
values assumed by these (random) variables; thus, for example,
{ -f
f} stands for {F, - fi,i, (i, ) E Zm }. We regard F as a
sample realization of a random field, usually isotropic and
homogeneous, and with significant correlations well beyond
nearest neighbors. Specifically, we model F as an MRF, or,
what is the same (see Section IV), we assume that the probability law of F is a Gibbs distribution. Given a neighborhood
(i, j) E Zm }, where Yi j C Zm denotes the
system .f =
neighbors of (i, /), an MRF over (Zm, Jf) is a stochastic process
indexed by Zm for which, for every (i, j) and every f,
=

-P(Fi, i =fis jI -F5k
=

I =

fk, 1, (k, 1)

(,j))

P(ffI, =fi, j S,k, I =.fk, 1,(k, 1) Ez Yi, j)

(1.1)

The MRF-Gibbs equivalence provides an explicit formula for
the joint probability distribution P(F =f) in terms of an energy function, the choice of which, together with f, supplies
a powerful mechanism foi- modeling spatial continuity and
oth r scene features.
The relaxation algorithm is designed to maximize the conditional probability distribution of (F, LX) given the data G =g
i.e., find the mode of the posterior distribution P(X = xI t; =
g). This form of Bayesian estimation is known as maximum
a posferiori or MAP estimation, or sometimes as penalized
maximum likelihood because one seeks to maximize log P(G =
gt x- x) + log P( X x) as a function of x; the second term is

the "penalty term." MAP estimation has been successfully
employed in special settings (see, e.g., Hunt [31] and Hansen
and Elliott [25] ) and we share the opinion of many that the
MAP formulation (and a Bayesian approach in general; see also
[24], [43], [45] ) is well-suited to restoration, particularly for
handling general forms of spatial degradation. Moreover, the
distribution of ('7 itself need not be known, which is fortunate
due to its usual complexity. On the other hand, MAP estimation clearly presents a formidable computational problem.
The number of possible intensity images is Lm2, where L denotes the number of allowable gray levels, which rules out any
direct search, even for small (m = 64), binary (L = 2) scenes.
Consequently, one is usually obliged to make simplifying
assumptions about the image and degradation models as well
as compromises at the computational stage. Here, the computational problem is overcome by exploiting the pivotal observation that the posterior distribution is again Gibbsian with
approximately the same neighborhood system as the original image, together with a sampling method which we call
the Gibbs Sampler. Indeed, our principal theoretical contribution is a general, practical, and mathematically coherent
approach for investigating MRF's by sampling (Theorem A),
and by computing modes (Theorem B) and expectations
(Theorem C).
The Gibbs Sampler generates realizations from a given MRF
by a "relaxation" technique akin to site-replacement algorithms in statistical physics, such as "spin-flip" and "exchange"
systems. The prototype is due to Metropolis et al. [42]; see
also [7] , [18], and Section X. Cross and Jain [12] use one of
these algorithms invented for studying binary alloys. ("Relaxation labeling" in the sense of [13], [30], [46], [47] is
different; see Section X.) The Markov property (1.1) permits
parallel updating of the line and pixel sites, each of which is
"refreshed" according to a simple recipe determined by the
governing distribution. Thus, both parts of the MRF-Gibbs
equivalence are exploited, for computing and modeling, respectively. Moreover, minimum mean-square error (MMSE)
estimation is also feasible by using the (temporal) ergodicity of
the relaxation chain to compute means w.r.t. the posterior distribution. However, we shall not pursue this approach.
We have used a comparatively slow, raster scan-serial version
of the Gibbs Sampler to generate images and restorations (see
Section XIII). But the algorithm is parallel; it could be executed in essentially one-half the time with two processors running simultaneously, or in one-third the time with three, and
so on. The full parallel potential is realized by assigning one
(simple) processor to each site of the intensity process and to
each site of the line process. Whatever the number of processors, parallel implementation is made feasible by a small
communications requirement among processors. The communications burden is related to the neighborhood size of the
graph associated with the image model, and herein lies much
of the power of the hierarchical structure: although the field
model X = (F, L) has a local graph structure, the marginal
distribution on the observable intensity process F has a completely connected graph. The introduction of a hierarchy
dramatically expands the richness of the model of the observed process while only moderately adding to the computa-

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

tional burden. We shall return to these points in Sections IV
and XI.
The MAP algorithm depends on an annealing schedule,
which refers to the (sufficiently) slow decrease of a ("control") parameter T that corresponds to temperature in a physical system. As T decreases, samples from the posterior distribution are forced towards the minimal energy configurations;
these correspond to the mode(s) of the distribution. Theorem
B makes this precise, and is, to our knowledge, the first theoretical result of this nature. Roughly speaking, it says that if
the temperature T(k) employed in executing the kth site replacement (i.e., the kth image in the iteration scheme) satisfies
the bound

log(1 +k)
for every k, where c is a constant independent of k, then with
probability converging to one (as k -*co), the configurations
generated by the algorithm will be those of minimal energy.
Put another way, the algorithm generates a Markov chain
which converges in distribution to the uniform measure over
the minimal energy configurations. (It should be emphasized
that pointwise convergence, i.e., convergence with probability
one, is in general not possible.) These issues are discussed in
Section XII, and the algorithm is demonstrated in Section XIII
on a variety of degraded images. We also discuss the nature of
the constant c in regard to practical convergence rates. Basically, we believe that the logarithmic rate is best possible.
However, the best (i.e., smallest) value of c that we have obtained to date (see the Appendix) is far too large for computational value and our restorations are actually performed with
small values of c. As yet, we do not know how to bring the
theory in line with experimental results in this regard.
The role of the Gibbs (or Boltzmann) distribution, and other
notions from statistical physics, in the construction of "expert
systems" is expanding. To begin with, we refer the reader
to [21] for the original formulation of our computational
method and of a general approach to expert systems based on
maximum entropy extensions. As previously mentioned,
Cerny [8] and Kirkpatrick et al. [40] introduced annealing
into combinatorial optimization. Other examples include the
work of Cheeseman [9] on maximum entropy and diagnosis
and of Hinton and Sejnowski [29] on neural modeling of inference and learning.
This paper is organized as follows. The degradation model is
described in the next section, and the undegraded image models are presented in Section IV after preliminary material on
graphs and neighborhood systems in Section IIl. In particular,
Section IV contains the definitions of MRF's, Gibbs distributions, and the equivalence theorem. Due to the plethora of
Markovian models in the literature, we pause in Section V to
compare ours to others, and in Section VI to explain some
connections with maximum entropy methods. In Section VII
we raise the issues of parameter estimation and model selection, and indicate why we are avoiding the former for the time
being. The posterior distribution is computed in Section VIII
and the corresponding optimization problem is addressed in
Section IX. The concept of stochastic relaxation is reviewed

723

in Section X, including its origins in physics. Sections XI and
XII are devoted to the Gibbs Sampler, dealing, respectively,
with its mechanical and mathematical workings. Our experimental results appear in Section XIII, followed by concluding
remarks.
II. DEGRADED IMAGE MODEL

We follow the standard modeling of the (intensity) image
formation and recording processes, and refer the reader to
[31] or [471 for better accounts of the physical mechanisms.
Let H denote the "blurring matrix" corresponding to a shiftinvariant point-spread function. The formation of F gives rise
to a blurred image H(F) which is recorded by a sensor. The
latter often involves a nonlinear transformation of H(F),
denoted here by 0, in addition to random sensor noise N =
{i, 4j, which we assume to consist of independent, and for
definiteness, Gaussian variables with mean , and standard deviation a.
Our methods apply to essentially arbitrary noise processes
N i= {rji,j}, discrete or continuous. However, computational
feasibility requires that the description of N as an MRF (this
can always be done; see Section IV) has an associated graph
structure that is approximately "local"; the same requirement
is applied to the image process X = (F, L). For clarity, we
forgo full generality and focus on the traditional Gaussian
white noise case. Extension to a general noise process is
mostly a matter of notation.
The degraded image is then a function of O(H(F)) and N, -say
P(k(H(F)), N), for example, addition or multiplication. (To
compute the posterior distribution, we only need to assume
that b -+ t(a, b) is invertible for each a.) For notational ease,
we will write
G = q(H(F))ON.
At the pixel level, for each (i, i) E Zm,

Gi,j=o

£ H(i- k,j- I)Fk,l)

(k, I)

(2.1)
71ij-

(2.2)

The mathematical results require an additional assumption,
namely, that F 'and N be independent as stochastic processes
(and likewise for L and N) and we assume this henceforth.
This is customary, although we recognize the limitation in certain contexts, e.g., for nuclear scan pictures.
For computational purposes, the degree of locality of F
should be approximately preserved by (2.1), so that the neighborhood systems for the prior and posterior distributions on
(F, L) are comparable. This is achieved when H is a simple
convolution over a small window. For instance, take

H(k,1)=

-D

16

k=0,l=0

Ikl, |I A 1, (k, 1) *(0, 0)

(2.3)

so that the intensity at (i, j) is weighted equally with the average of the eight nearest neighbors. The function 0 is unrestricted, bearing in mind that the true noise level depends on
0, 0, and a. Typically, 0 is logarithmic (film) or algebraic

(TV).

An important special case, which occurs in two-dimensional
(2-D) signal theory, is the segmentation of noisy images into

724

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

coherent regions. The usual model is
(2.4)

00

0.000

0o
0 00
0
0
0.0 0

where N is white noise and the number of intensity levels is
small. This is the model entertained by Hansen and Elliott
[25] for simple, binary MRF's F, and by many other workers
with varying assumptions about F; see [14], [16], [17]. In
this case, namely (2.4), we can extract simple images under extremely low signal-to-noise ratios.
The full degraded image is (G, L); that is, the "line process"
is not transformed.

0

0 00

00 00 0

0=1
C=l

0=2
C =2

~~~0=10
C= 8

(a)

tb)

(c)

G= F + N

0 0-U

(e)

(d)

III. GRAPHS AND NEIGHBORHOODS

Here and in Section IV we present the general theory of
MRF's on graphs, focusing on the aspects and examples which
figure in the experimental restorations. The level of abstraction is warranted by the variety of MRF's, graphs, and probability distributions simultaneously under discussion.
Let S = {s, 2, * * *, SN} be a set of sites and let = S
s E S} be a neighborhood system for S, meaning any collection of subsets of S for which 1) s 0 gs and 2) s EG , r E
gs. Obviously, gs is the set of neighbors of s and the pair
{S, g } is a graph in the usual way. A subset C C S is a clique
if every pair of distinct sites in C are neighbors; e denotes the
set of cliques.
The special cases below are especially relevant.
Case 1: S = Zm This is the set of pixel sites for the intensity
process F; {Sl, S2, * * *, sN}, N.= m2, is any ordering of the
lattice points. We are interested in homogeneous neighborhood systems of the form
Yc= {:i,ji(ii ) cZm ; J: , j

= {(k, 1) eZm :0< (k- i)2 + (1- )2 c}.

Notice that sites at or near the boundary have fewer neighbors
than interior ones; this is the so-called "free boundary" and is
more natural for picture processing than torodial lattices and
other periodic boundaries. Fig. 1(a), (b), (c) shows the (interior) neighborhood configurations for c = 1, 2, 8; c = 1 is the
first-order or nearest-neighbor system common in physics, in
which Yi, i= {(i, j - I), (i,j+ 1), (i- l,5j), (i + l, j)},with adjustments at the boundaries. In each case, (i, j) is at the center, and the symbol o stands for a neighboring pixel. The
cliques for c = 1 are all subsets of Zm of the form {(i, j)},
{(i, j), (i, f + 1)} or {(i, j), (i + 1, j)}, shown in Fig. 1(d). For
c = 2, we have the cliques in Fig. 1(d) as well as those in Fig.
1(e). Obviously, the number of clique types grows rapidly
with c. However, only small cliques appear in the model for
F actually employed in this paper; indeed, the degree of progress with only pair interactions is somewhat surprising. Nonethel-ss, more complex images will likely necessitate more complex energies. Our experiments (see Section XIII) suggest that
much of this additional complexity can be accommodated
while maintaining modest neighborhood sizes by further developing the hierarchy.
Case 2: S = Dm, the "dual" m X m lattice. Think of these
sites as placed midway between each vertical or horizontal pair
of pixels, and as representing the possible locations of "edge

X o

0

x

x

0

x

O XO XO
(f)

oo
B

Fig. 1.

0

0

0

(g)

elements." Shown in Fig. l(f) are six pixel sites together with
seven line sites denoted by an X. The six surrounding X's are
the neighbors of the middle X for the neighborhood system we
denote by 2 = {fd, d E Dm }. Fig. 1 (g) is a segment of a realization of a binary line process for which, at each line site,
there may or may not be an edge element. We also consider
line processes with more than two levels, corresponding to
edge elements with varying orientations.
Case 3: S = Zm U Dmi. This is the setup for the field (F, L).
Zm has neighborhood system Y1 (nearest-neighbor lattice) and
Dm has the above-described system. The pixel neighbors of
sites in Dm are the two pixels on each side, and hence each
(interior) pixel has four line site neighbors.
IV. MARKOV RANDOM FIELDS AND
GIBBS DISTRIBUTIONS
We now describe a class of stochastic processes that includes
both the prior and posterior distribution on the original image.
In general, this class of processes (namely, MRF's) is neither
homogeneous nor isotropic, assuming the index set S has
enough geometric structure to even define a suitable family of
translations and rotations. However, the particular models we
choose for prior distributions on the original image are in fact
both homogeneous and isotropic in an appropriate sense.
(This is not the case for the posterior distribution.) We refer
the reader to Section XIII for a precise description of the prior
models employed in our experiments, and in particular for spe-

cific examples of the role of the line elements.
As in Section III, {S, G} denotes an arbitrary graph. Let
X = {X, s E S} denote any family of random variables indexed by S. For simplicity, we can assume a common state
space, say A_ {0, 1, 2,- * * ,L - I}, so that XSEA for all s;
the extension to site-dependent state spaces, appropriate when
S consists of both line and pixel sites, is entirely straightforward (although not merely a notational matter due to the
"positivity condition" below). Let Q be the set of all possible
configurations:
Q2 = {co = (xsl, * *. 1 xsN): xs, GE A, I < i <N}.

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

As usual, the event {Xsl = xS1, , XSN = XSN} is abbreviated
{X = (}.
X is an MRF with respect to g if

P(X = )>0

for all co C Q;

(4.1)

Xr = xr,r S) =P(XS= Xs Xs = Xr rr E s)
P(Xs= xs1

(4.2)
for every s E S and (xs1, * , xN) E Q2. Technically, what is
meant here is that the pair { X, P} satisfies (4.1) and (4.2) relative to some probability measure on Q2. The collection of
functions on the left-hand side of (4.2) is called the local characteristics of the MRF and it turns out that the (joint) probability distribution P(X = c) of any process satisfying (4.1) is
uniquely determined by these conditional probabilities; see,
e.g., [6, p. 195].
The concept of an MRF is essentially due to Dobrushin [15]
and is one way of extending Markovian dependence from 1 -D
to a general setting; there are, of course, many others, some of
which will be reviewed in Section V.
Notice that any X satisfying (4.1) is an MRF if the neighborhoods are large enough to encompass the dependencies. The
utility of the concept, at least in regard to image modeling, is
that priors are available with neighborhoods that are small
enough to ensure feasible computational loads and yet still
rich enough to model and restore interesting classes of images
(and textures: [12]).
Ordinary 1-D Markov chains are MRF's relative to the
nearest-neighbor system on S ={1, 2, - --, N} (i.e., 0l = {2},
i= {i - 1, i + 1 } 2 A i < N - 1, AN = {N - 41) if we assume
all positive transitions and the chain is started in equilibrium.
In other words, the "one-sided" Markov property

P(Xk =XkfXj =xj,i<k- 1)=P(Xk =XklXkl-Xk-l)
and the "two-sided" Markov property
= Xj, j V k) = P(Xk = Xk Xj Xj,iE
=
k)
P(Xk =Xk IXi
are equivalent. Similarly for an rth order Markov process on

the line with respect to the r nearest neighbors on one side and
on both sides. (This appears to be doubted in [I] but follows,
eventually, from straightforward calculations or immediately
from the Gibbs connection.)
Gibbs models were introduced into image modeling by
Hassner and Sklansky [28], although the treatment there is
mostly expository and limited to the binary case.
A Gibbs distribution relative to {S, g} is a probability measure ir on Q. with the following representation:

eU(w)IT
7r(co) = z

(4.3)

where Z and Tare constants and U, called the energy function,
is of the form

UM c= E VC
c c(

.

(4.4)

Recall that C denotes the set of cliques for g. Each Vc is a
function on 2 with the property that Vc(co) depends only
on those coordinates x5 of co for which s E C. Such a fam-

725

ily { Vc, CC C} is called a potential. Z is the normalizing
constant:
Z *

v

(4.5)

e-U(()/T

-L

and is called the partition function. Finally, T stands for
"temperature"; for our purposes, T controls the degree of
"peaking" in the "density" ir. Choosing T "small" exaggerates
the mode(s), making them easier to find by sampling; this is
the principle of annealing, and will be applied to the posterior
distribution ir(f, 1) = P(F = f, L = I GG = g) in order to find the
MAP estimate. Of course, we will show that ir(f, 1) is Gibbsian
and identify the energy and neighborhood system in terms of
those for the priors. The choice of the prior distributions, i.e.,
of the particular functions Vc for the image model r(Co) =
P(X = c), will be discussed later on; see Section VII for some
general remarks and Section XIII for the particular models employed in our experiments.
The terminology obviously comes from statistical physics,
wherein such measures are "equilibrium states" for physical
systems, such as ferromagnets, ideal gases, and binary alloys.
The Vc functions represent contributions to the total energy
from external fields (singleton cliques), pair interactions
(doubletons), and so forth. Most of the interest there, and in
the mathematical literature, centers on the case in which S is
an infinite, 2-D or 3-D lattice; singularities in Z may then
occur at certain ("critical") temperatures and are associated
with "phase transitions."
Typically, several free parameters are involved in the specification of U, and Z is then a function of those parametersnotoriously intractable. For more information see [3 ], [5],

[6], [23], [32],and [39].

The best-known of these lattice systems is the Ising model,
invented in 1925 by E. Ising [33] to help explain ferromagnetism. Here, S = Zm and 9 = , the nearest-neighbor system.
The most general form of U is then
1

U(c) = ZVpi,m}(x1,j) + ZV{(fUJ), (i+,j)}(Xi,j,Xi+1,j)
+

(4.6)

f 0, D, (i,j+ 1)}(xi, jxi,x+ )

where the sums extend over all (i, j) E Zm for which the indicated cliques make sense. The Ising model is the special case
of (4.6) in which X is binary (L = 2), homogeneous (= strictly
stationary), and isotropic (= rotationally invarient):

u((.,)=a EXi,ij+ ( xi jxi+1 j+ Zxi,jxi,j+1) (4.7)

for some parameters al and f, which measure, respectively, the
external field and bonding strengths.
Returning to the general formulation, recall that the local
characteristics

1T(XsIXr, r :$ s) = 1E(CA)

s E S, X

EQ

x5 E A

uniquely determine ir for any probability measure 7r on Q,
r(co) > 0 for all co. The difficulty with the MRF formulation
by itself is that
i) the joint distribution of the Xs is not apparent;

726

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

ii) it is extremely difficult to spot local characteristics, i.e.,
to determine when a given set of functions V(xsJXr, r $
S), S E S, (xsl, - - , xsN) C Q2, are conditional probabilities for some (necessarily unique) distribution on Q.
For example, Chellappa and Kashyap [10] allude to i) as a
disadvantage of the "conditional Markov" models. See also
the discussion in [6] . In fact, these apparent limitations to
the MRF formulation have been noted by a number of authors,
many of whom were obviously not aware of the following
theorem.
Theorem: Let q be a neighborhood system. Then X is an
MRF with respect to 9 if and only if nT(c) =P(X = co) is a
Gibbs distribution with respect to 9.
Among other benefits, this equivalence provides us with a
simple, practical way of specifying MRF's, namely by specifying potentials, which is easy, instead of local characteristics,
which is nearly impossible. In fact, with some experience, one
can choose U's in accordance with the desired local behavior,
at least at the intensity level. In short, the modeling and consistency problems of i) and ii) are eliminated.
Proofs may be found in many places now; see, e.g., [39] and
the references therein, or the approach via the HammersleyClifford expansion in [6]. An influential discussion of this
correspondence appears in Spitzer's work, e.g., [48] . Explicit
formulas exist for obtaining U from the local characteristics.
Conversely, the local characteristics of 1r are obtained in a
straightforward way from the potentials: use the defining
ratios and make the allowable cancellations. Fix s E S, cX =
(Xsl X * , XsN) C Q, and let wX denote the configuration
which is x at site s and agrees with co everywhere else. Then if
7r(w) = P(X = co) is Gibbsian,

P(Xs=xsIXr=xrrs)=Zsexp T C: sEC Vc(Q -)
(4.8)

Zs- E exp
x EA

(4.9)

E Vc(wX).
T C:sGC

Notice that the right-hand side of (4.8) only depends on xS
and on xr, r C 9s, since any site in a clique containing s must
be a neighbor of s. These formulas will be used repeatedly to
program the Gibbs Sampler for local site replacements.
For the Ising model, the conditional probability that Xi j=
xi,], given the states at S\{i, j}, or equivalently, just the four
nearest neighbors, reduces to

the line process L, expands the graph structure of the marginal
distribution of the intensity process F. Consider first an arbitrary MRF X with respect to a graph {S, 9 }. Fix r E S and let
X = {Xs, s C S, s r}. The marginal distribution P of X is derived from the distribution P of X by summing over the range
of Xr. Use the Gibbs representation for P and perform this
summation: the resulting expression for P can be put in the
Gibbs form, and from this the neighborhood system on S
S\{r} can be inferred. The conclusion of this exercise is that
S1, S2 CS are, in general, neighbors if either i) they were
neighbors in S under 9 or ii) each is a neighbor of r E S under
9. Now let X = (F, L), with neighborhood system defined at
the end of Section III. Successive summations of the distribution of X over the ranges of the elements of L yields the marginal distribution of the observable intensity process F. Each
summation leaves a graph structure associated with the marginal distribution of the remaining variables, and this can be related to the original neighborhood system by following the
preceding discussion of the general case. It is easily seen that
when all of the summations are performed, the remaining
graph is completely connected; under the marginal distribution
of F, all sites are neighbors. This calculation suggests that significant long-range interactions can be introduced through the
development of hierarchical structures without sacrificing the
computational advantages of local neighborhood systems.
=

V. RELATED MARKOV IMAGE MODELS
The use of neighborhoods is, of course, pervasive in the literature: they offer a geometric framework for the clustering of
pixel intensities and for many types of statistical models. In
particular, the Markov property is a natural way to formalize
these notions. The result is a somewhat bewildering array of
Markov-type image models and it seems worthwhile to puase
to relate these to MRF's. The process under consideration
is F = {Fi,j, (i, j) C Zm}, the gray levels, or really any pixel
attribute.
An early work in this direction is Abend, Harley and Kanal
[1] about pattern classification. Among many novel ideas,
there is the notion of a Markov mesh (MM) process, in which
the Markovian dependence is causal: generally, one assumes
that, for all (i, j) and f,

P(Fi,i = fi1, i= f,,(k, IE(Ak, j)
= P(Fi, = fi,
i
IFk, I= fk,1, (k,1) GBi, j)

(5.1)

vwsiere.B j CAi,jC{(k,l):k<iorl<j}. A common example is Bi, = {(i - 1, j), (i - 1, j- 1), (i, j - 1)}. Besag [6],
Kanal [37], and Pickard [44] also discuss such "unilateral"
e-Xij(a + Vi, ti)
processes, which are usually a subclass of MRF's, although the
1 +( O+ ti, i)
resulting (bilateral) neighborhoods can be irregular. Anyway,
1
where vi,j = xi,H
This is also for MM models the emphasis is on the causal, iterative aspects,
+x1,X+i +x1+
j
known as the autologistic model and has been used for texture including a recursive representation for the joint probabilities.
modeling in [12]. More generally, if the local characteristics Incidentally, a Gibbs type description of rth order Markov
are given by an exponential family and if Vc(&) 0 for C| > chains is given in [1]; of course, the full Gibbs-MRF equiva2, then the pair potentials always "factor" into a product of lence is not perceived and was not for about five years. Derin
et al. [14] model Fl as an MM process and use recursive Bayes
two like terms; see [6].
We conclude with some further discussion of a remark made smoothing to recover F from a noisy version F} + N; the algoin Section 1: that the hierarchical structure introduced with rithms exploit the causality to maximize the univariate poste-

I

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

rior distribution at each pixel based on the data over a strip
containing it, and are very effective at low S/N ratios for some

727

[11], who use a Markov boundary model and a deterministic
relaxation scheme.
simple images.
VI. MAXIMUM ENTROPY RESTORATION
Motivated by a paper of Levy [41], Woods [51] defined
There are several contact points. The Gibbs distribution can
"P-Markov" processes for the resolution of wavenumber specbe
derived (directly from physical principles in statistical metra. The definition involves two spatial regions separated by a
by maximizing entropy: basically, it has maximal enchanics)
"boundary" of width P, and correspond to the past, future,
among
all probability measures (equilibrium states) on
tropy
and present in 1-D. Woods also considers a family of "widewith
the
same
average energy. Thus it is no accident that,
Q
sense" Markov fields of the form
like maximum entropy (ME) methods, ours are well-suited to
(5.2) nonlinear problems; see [50] . Moreover, based on the success
ok, IFi-k,j- I+ Ui,j
Fi,
(k, I) E Wp
of ME restoration (along, the lines suggested by Jaynes [36] )
where Wp = {(k, 1):0< k2 +12 P}, Ok,I are the MMSE co- for recovering randomly pulsed objects (cf. Frieden [19] ), we
efficients for projecting Fi, i on {Fk, (k, 1) E (i, j) + Wp}, intend in, the future to analyze such data (e.g., starfield photoand {Ui,j} is the error, generally nonwhite. The main theo- graphs) by our methods.
We should also like to mention the interesting observation of
retical result is that if {Ui,j} is homogeneous, Gaussian, and
satisfies a few other assumptions, then F is Gaussian, P-Markov Trussell [50] that conventional ME restoration is a special case
and vice-versa. In general, there are consistency problems of MAP estimation in which the prior distribution on F is
and the P-Markov property is hard to verify. In the nearestP(F = f) = exp (-,3 fi, log fi, ) (normalizing constant).
neighbor case, one gets a Gaussian MRF.
Other "wide-sense" Markov processes appear in Jain and By "conventional ME," we refer to maximizing the entropy
Angel [35] and Stuller and Kurz [49]. The assumptions
fi, i log fi,i subject to E r2b = constant (-, is here again
the
noise process); see [2]. Other ME methods (e.g., [19]) do
in [35] are a nearest-neighbor system, white noise, and no
not
appear to be MAP-related.
blur; restoration is achieved by recursively filtering the rows
{Fi, i 1'=, which form a vector-valued, second-order Markov
VII. MODEL SELECTION AND PARAMETER ESTIMATION
chain, to find the optimal interpolator of each row. In [49],
The quality of the restoration will clearly depend on choices
causality is introduced and earlier work is generalized by conmade at the modeling stage, in our case about specific energy
sidering an arbitrary "scanning pattern."
The "spatial interaction models" in Chellappa and Kashyap types, attribute processes, and parameters. Cross and Jain
[10], [38] satisfy (5.2) for general coefficients and W's. The [12] use maximum likelihood estimation in the context of
model is causal if W lies in the third quadrant. The authors Besag's [6] "coding scheme," as well as standard goodness-ofconsider "simultaneous autoregressive" (SAR) models, wherein fit tests, for matching realizations of autobinomial MRF's to
the noise is white, and "conditional Markov" (CM) models, real textures. Kashyap and Chellappa [38] introduce some
wherein the "bilateral" Markov property holds (i.e., (1.1) with new methods for parameter estimation and the choice of
Yi, i = (i, j) + W) in addition to (5.2), and the noise is non- neighborhoods for the SAR and CM models, mostly in the
white. Thus, the CM models are MRF's, although in [10], Gaussian case. These are but two examples.
For uncorrupted, simple MRF's, the coding methods do
[38] the boundary of Zm is periodic, and hence boundary
conditions must be adjoined to (5.2). Given any (homoge- finesse the problem of the partition function. However, for
neous) SAR process there exists a unique CM process with the more complex models and for corrupted data, we feel that the
same spectral density, although different neighborhood struc- coding methods are ultimately inadequate due to the complexture. The converse holds in the Gaussian case but is generally ity of the distribution of G. This view seems to be shared by
false (see the discussion in Besag [6]). MMSE restoration of other authors, although in different contexts. Of course, for
blurred images with additive Gaussian noise is discussed in MRF's, the obstacles facing conventional statistical inference
due to Z have often been noted. Even for the Ising model,
[10] ; the original image is SAR or CM, usually Gaussian.
results are rare; a famous exception is Onsager's
deElliott
analytical
and
et
al.
Elliott
[17]
Hansen
and
[25]
Finally,
work
on
the
correlational structure.
of
sensed
for
the
MAP
remotely
segmentation
algorithms
sign
At any rate, we have developed a new method [20] for estidata with high levels of additive noise. The image model is a
nearest-neighbor, binary MRF. However, the autologistic form mating clique parameters from the "noisy" data, and this will
of the joint distribution is not recognized due to the lack of be implemented in a forthcoming paper. For now, we are
the Gibbs formulation. The conditional probabilities are ap- obliged to choose the parameters on an ad hoc basis (which is
proximated by the product of four 1 -D transitions, and seg- common), but hasten to add that the quality of restoration
mentation is performed by dynamic programming, first for does not seem to have been adversely affected, probably due
each row and then for the entire images. More recent work in to the relative simplicity of the MRF's we actually use for the
Elliott et al. [16] is along the same lines, namely MAP esti- line and intensity processes; see Section XIII.
One should also address the general choice of ir and . This
mation, via dynamic programming, of very noisy but simple
is
really quite different than parameter estimation and someof
the
Gibbs
formulathe
use
the
differences
are
major
images;
tion and improvements in the algorithms. Similar work, ap- what related to "image understanding": how does one incorplied to boundary finding, can be found in Cooper and Sung porate "real-world knowledge" into the modeling process? In
=

l,

7 "8

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

image interpretation systems, various semantical and hierarchiP(G=gfX =w)P(X=w)
P(x WI c =g)=(8.3)
cal models have been proposed (see, e.g., [26]). We have
= g)
P(i
begun our study of hierarchical Gibbs models in this paper. A
general theory of interactive, self-adjusting models that is prac- for all X = (f, 1), for each g.
Since P( = g) is a constant and P( X = w) = eU(w)/Z, the
tical and mathematically coherent may lie far ahead.
key term is
VIII. POSTERIOR DISTRIBUTION
P( (, = g\=w) = P(H(H( t0D`N
)) = gI 1-' f,1= 1)
We now turn to the posterior distribution P( -l f, L 1=
= P(QN = ((g, O(H(f))) |= f, 1=1)
g) of the original image given the "data" g. In this section we
take S = Z1n U Dmi, the collection of pixel and line sites, with
= P(N = 'I(g, q(H(f))))
some neighborhood system 9 {= , s ES}; an example of
such a "mixed" graph was given in Section III. The configura- (since N is independent of IF and 1)
tion space is the set of all pairs w = (f, 1) where the components of f assume values among the allowable gray levels and
(27ra 2)M/2 exp (-2) fu - f112
those of I among the (coded) line states.
We assume that \ is an MRF relative to {S, 9 } with corre- We will write (F for (F(g,
O(H(f))). Collecting constants we
sponding energy function U and potentials { Vc}:
have, from (8.3),
P(X--F f1 = 1) = e-Uff, I)ITIZ
P(X = (', = g) = e UP(w)/ZP
Z Vc(f,l)
U(f, ) L
for UP as in (8.2); Zp is the usual normalizing constant (which
c
will depend on g). It remains to determine the neighborhood
For convenience, take T I
structure.
Recall that CY -(H(F)) (0l , where N is white Gaussian
Intuitively, the line sites should have the same neighbors
noise with mean , and variance a2 and is independent of X.
whereas the neighbors 9. of a pixel site s C Zm should be augWe emphasize that what follows is easily extended to pro- mented in accordance with the blurring mechanism.
Take s C Din. The local characteristics at s for the posterior
cesses {N that are more general MRF's, although we still require
that N be independent of N. The operation 0 is assumed in- distribution are, by (8.2),
vertible and we will write fN = O(F(,f(H(F)))= {4, s CZ
=f, C Dg)
s
P(Ls=lsILr=lr,r7 srCDm,
to indicate this inverse.
Let J , s C Zm, denote the pixels which affect the blurred
image H( 1") at s. For instance, for the H in (2.3), Rs is the 3 X
E e-UP(f )
ZE eU(ff, 1)
3 square centered at s. Observe that 45s, s C Zm, depends only
is
Is
on g, and {tf, t E Hs}. By the shift-invariance of H, J(r+s =
s + Jr where JR C Z, s + r CZm, and s + {r is understood to where the sum extends over all possible values of Ls. Hence
be intersected with Zm, if necessary. In addition, we will asss
sumne that {i(s} is "symmetric" in that r C J(%> -r C Ho0.
For s CZM, the term in (8.2) involving (F does not cancel
Then the collection {J{s\{s}, s EZm} is a neighborhood sys- out. Now (D(g, 4(H(f))) = {(Fs, s C Zm} and let us denote the
tem over Z, Let H2 denote the second-order system, i.e.,
dependencies in (Ds by writing (F, = (Ds(gs;ft, t CE Ys). Then
=

91

=

-e~u(b*

y2S

U

r&

Rs~

Jr,

P(F, f=5 =Fr r r s,r C Z,1,= 1

ScZm

Then it is not hard to see that { J2 \ {s}, s C Zm } is also a neighborhood system. Finally, set 9P = {9P, s C S} where
qp
"S

,c

s E Dm

.S.
=

'S U 12\{s},

(8.1)

sCZm.

The "P" stands for "posterior"; some thought shows that 9 p
is a neighborhood system on S.
C M(M = N2) have all components = p and let *
Let .tE
denote the usual norm in RlM: |V||2 = 1M V2
Theorem: For each g fixed, P(X =
7 = g) is a Gibbs distribution over {S, 9p} with energy function

1 1

(I

UP(f, I) = U(f, l) + I au (F(g, O(H(f ))) | 2/2 U2.
-

e-U(fl)

(8.2)

Proof: Using standard results about "regular conditional
expectations," we can and do assume that

-upff, I)

=g)

(t )

e-UP(f, )
fs

U(f, I) + Z (r - p)V/2uJ2.

(8.4)

r E- Zm

Decompose UP as follows:

U.P(f, ) = C:S&C- VC(f, I)
+ (2u72)
+

Z
C: sc

+ (2or2)-1

t
Z1E ((Dr(gr;ft,tJr)

r : s E Hr

u)2

Vc(f, l)
, (4,(g,; ft, t E Jr) - p)2.
r:s1 J,r

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

Since the last two terms do not involve f, (remember that Vc
only depends on the sites in C), the ratio in (8.4) depends only
on the first two terms above. The first term depends only on
coordinates of (f, 1) for sites in 9,(s C Ce CC g) and the
second term only on sites in
U

r:sEGr

H{r

U

rE S

{r-Js

coIg)

jItt

(g, k(H(f))) I2/2u2,

(f, 1) C Q.

(9.1)

where (see Section VIII) 1 is defined by b(H(f)) 0 ) = g.
Even without L, the size of Q2 is at least 24000, corresponding
to a binary image on a small (64 X 64) lattice. Hence, the
identification of even near-optimal solutions is extremely difficult for such a relatively complex function.
In Sections XI and XII we will describe our stochastic relaxation method for this kind of optimization. The same method
works for sampling and for computing expectations (and
hence forming likelihood ratios), as will be explained in Section XI. The algorithm is highly parallel, but our current
implementation is serial: it uses a single processor. The restoration of more complex images than those in Section XIII,
probably involving more levels in the hierarchy, may necessitate some parallel processing.
X. STOCHASTIC RELAXATION
There are many types of "relaxation," two of them being
the type used in statistical physics and the type developed in
image processing called "relaxation labeling" (RL), or sometimes "probabilistic relaxation." Basically, ours is of the former class, referred to here as SR, although there are some common features with RL.
The "Metropolis algorithm" (Metropolis et al. [42]) and
others like it [7], [18] were invented to study the equilibrium

properties, especially ensemble averages, time-evolution, and
low-temperature behavior, of very large systems of essentially
identical, interacting components, such as molecules in a gas or
atoms in binary alloys.
Let Q2 denote the possible configurations of the system; for
example, co C Q might be the molecular positions or site configuration. If the system is in thermal equilibrium with its
surroundings, then the probability (or "Boltzmann factor") of
co is given by

7r(c) = e-08(w) E e-$6(U),
(A

E Y(co)e fI(Q)

[

IX. THE COMPUTATIONAL PROBLEM
is a powerful tool for
The posterior distribution P(X =
image analysis; in principle, we can construct the optimal
(Bayesian) estimator for the original image, examine images
sampled from P(\ = cog), estimate parameters, design nearoptimal statistical tests for the presence or absence of special
objects, and so forth. But a conventional approach to any of
these involves prohibitive computations. Specifically, our job
here is to find the value(s) of co which maximize the posterior
distribution for a fixed g, i.e., minimize

U(f, l)

where d (X) is the potential energy of X and ,B = 1/KT where
K is Boltzmann's constant and T is absolute temperature. We
have already seen an example in the Ising model (4.7). Usually,
one needs to compute ensemble averages of the form

(Y)= j Y(c)d(c)=
dE e-

Hence, g' = gs U J{2\{s}, as asserted in the theorem.

+

729

coCE2

where Y is some variable of interest. This cannot be done
analytically. In the usual Monte Carlo method, one restricts
the sums above to a sample of o's drawn uniformly from Q..
This, however, breaks down in the situation above: the exponential factor puts most of the mass of 7r over a very small part
of Q2, and hence one tends to choose samples of very low probability. The idea in [42] is to choose the samples from ir instead of uniformly and then weight the samples evenly instead
of by dir. In other words, one obtains co1, Co2, --* , CR from
1r and (Y) is approximated by the usual ergodic averages:
I

R

(Y)R 1
E

r= 1

Y(C,).)

(10.1)

Briefly, the sampling algorithm in [42] is as follows. Given
the state of the system at "time" t, say X(t), one randomly
chooses another configuration 7i and computes the energy
change A/ = & (7i) - & (X(t)) and the quantity
q

=

I
=
r(X()
)-A
e(t)

(10.2)

If q > 1, the move to 71 is allowed and X(t + 1) = rq, whereas if
q < 1, the transition is made with probability q. Thus we
. q and
choose 0 '< < 1 uniformly and set X(t + 1) = 71 if <
X(t + 1) = X(t) if t> q. (A "parallel processing variant" of
this for simulating certain binary MRF's is given by Berger
and Bonomi [4].)
In binary, "single-flip" studies, rq = X(t) except at one site,
whereas in "spin-exchange" [18] systems, a pair of neighboring sites is selected. In either case, the "flip" or "exchange"
is made with probability q/(1 + q), where q is given in (10.2).
In special cases, the single-flip system is equivalent to our
Gibbs Sampler. The exchange algorithm in Cross and Jain
[12] is motivated by work on the evolution of binary alloys.
The samples generated are used for visual inspection and statistical testing, comparing the real and simulated textures. The
model is an autobinomial MRF; see [6] or [12]. The algorithm is not suitable (nor intended) for restoration: for one
thing, the intensity histogram is constant throughout the iteration process. This is necessarily the case with exchange systems which depend heavily on the initial configuration.
The algorithm in Hassner and Sklansky [28] is apparently a
modification of one in Bortz et al. [7]. Another application
of these ideas outside statistical mechanics appears in Hinton
and Sejnowski [29], a paper about neural modeling but a spiritual cousin of ours. In particular, the parallel nature of these
algorithms is emphasized.
The essence of every SR scheme is that changes (co-l7q)
which increase energy, i.e., lower probability, are permitted.

730

IEEE TRANSACTIONS ON PAT fERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

By contrast, deterministic algorithms only allow jumps to states
of lower energy and invariably get "stuck" in local minima.
To get to samples from 1T, we must occasionally "backtrack."
All of these algorithms can be cast in a general theory involving Markov chains with state space Q2. See Hammersley and
Handscomb [271 for a readable treatment. The goal is an
irreducible, aperiodic chain with equilibrium measure 7r. If
WI cWI' ' XR is a realization of such a chain, then standard results yield (10.1), in fact at a rate O(R -1/2) as R -* o.
In this setup an auxiliary transition matrix is used to go from
to q, and the general replacement recipe involves the same
ratio r(i1)/ir(cu). The Markovian properties of the Gibbs Sampler will be described in the following sections.
Chemical annealing is a method for determining the low energy states of a material by a gradual lowering of temperature.
The process is delicate: if T is lowered too rapidly and insufficient time is spent at temperatures near the freezing point,
then the process may bog down in nonequilibrium states, corresponding to flaws in the material, etc. In simulated annealing, Kirkpatrick et al. [40] identify the solution of an optimal
(computer) design problem with the ground state of an imaginary physical system, and then employ the Metropolis algorithm to reach "steady-state" at each of a decreasing sequence
of teinperatures {Tn} This sequence, and the time spent at
each temperature, is called an "annealing schedule." In [40],
this is done on an ad hoc basis using guidelines developed for
c-hemical annealing. Here, we prove the existence of annealing
schedules which guarantee convergence to minimum energy
states (see Section XII for formal definitions), and we identify
the rate of decrease relative to the number of full sweeps.
Turning to RL, there are many similarities with SR, both in
purpose and, at least abstractly, in method. RL was designed
for the assignment of numeric or symbolic labels to objects in
a visual system, such as intensity levels to pixels or geometric
labels to cube edges, in order to achieve a "global interpretation" that is consistent with the context and certain "local
constraints." Ideally, the process evolves by a series of local
changes, which are intended to be simple, lhomogeneous, and
performed in parallel The local constraints are usually socalled 'compatibility functions," which are much like statistical correlations, and often defined in reference to a graph.
We refer the reader to Davis and Rosenfeld [13] for an expository treatment, to Rosenfeld et al. [46] for the origins, to
Hummel and Zucker [30] for recent work on the logical and
mathematical foundations, and to Rosenfeld and Kak [47] for
applications to iterative segmentation.
But there are also fundamental differences. First, most variants of RL are rather ad hoc and heuristic. Second, and more
:importantly, RL is essentially a nonstochastic process, both in
the interaction model and in the updating algorithms. (Indeed,
various probabilistic analogies are often avoided as misleading;
see [30], for example.) There is nothing in RL corresponding
to an equilibrium measure or even a joint probability law over
configurations, whereas there is no analogue in SR of the allimportant, iterative updating formulas and corresponding sequence of "probability estimates" for various hypotheses inVOlving pixel or object classification.
In summary, there are shared goals and shared features (lo-

X

cality, parallelism, etc.) but SR and RL are quite distinct, at
least as practiced in the references made here.
XI. GIBBS SAMPLER: GENERAL DESCRIPTION
We return to the general notation of Section TV: \ {= ,
s < S} is an MRF over a graph {g, s C S} with state spaces
As, configuration space Q = Is As, and Gibbs distribution
frQ(o) = e-U(w)IT/Z X E2.
The general computational problems are

A) sample from the distribution ar;
B) minimize U over Q;
C) compute expected values.
Of course, we are most concerned with B), which corresponds to MAP estimation when ir is the posterior distribution.
The most basic problem is A), however, because A) together
with annealing yields B) and A) together with the ergodic
theorem yields C). We will state three theorems corresponding
to A), B), and C) above. Theorem C is not used here and will
be proven elsewhere; we state it because of its potential importance to other methods of restoration and to hypothesis
testing.
Let us imagine a simple processor placed at each site s of the
graph. The connectivity relation among the processors is determined by the bonds: the processor at s is connected to each
processor for the sites in gs. In the cases of interest here (and
elsewhere) the number of sites N is very large. However, the
size of the neighborhoods, and thus the number of connections to a given processor, is modest, only eight in our experiments, including line, pixel and mixed bonds.
The state of the machine evolves by discrete changes and it
is therefore convenient to discretize time, say t - 1, 2, 3, -- .
At time t, the state of the processor at site s is a random variable Xs(t) with values in A,. The total configuration is X(t) a
(t), XS2 (t), * , XsN(t)), which evolves due to state
(XSI
changes of the individual processors. The starting configuration, X(0), is arbitrary. At each epoch, only one site undergoes a (possible) change, so that X(t - 1) and X(t) can differ in
at most one coordinate. Let ni, n2, . be the sequence in
which the sites are "visited"' for replacement; thus, nt F S and
Xsi(t) -Xsi(t - 1), i ' nt. Each processor is programmed to
follow the same algorithm: at time t, a sample is drawn from
the local characteristics of in for s = nt and co X(t - 1). In
other words, we choose a state x F Ant from the conditional
distribution of Xnt given the observed states of the neighboring sites Xr (t - 1), r F nt. The new configuration X(t) has
= x and Xs(t) = Xs(t - 1), s # nt.
Xn,(t)
These are local computations, and identical in nature when X
is homogeneous. Moreover, the actual calculation is trivial
since the local characteristics are generally very simple. These
conditional probabilities were discussed in Section IV and we
refer the reader again to formulas (4.8) and (4.9). Notice that
Z does not appear.
Given an initial configuration X(0), we thus obtain a sequence X(1), X(2), X(3), * of configurations whose convergence properties will be described in Section XII. The limits
obtained do not depend on X(0). The sequence (nt) we actually use is simply the one corresponding to a raster scan, i.e.,
=

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

repeatedly visiting all the sites in some "natural" fixed
Of course, in this case one does not actually need a pr
at each site. But the theorems are valid for very gene]
necessarily periodic) sequences (nt) allowing for asych
schemes in which each processor could be driven by,
clock. Let us briefly discuss such a parallel implementa
the Gibbs Sampler and its advantage over the serial versi
Computation is parallel in the sense that it is realized
ple and alike units operating largely independently. U
dependent only to the extent that each must transmit
rent state to its neighbors. Most importantly, the am
time required for one complete update of the entire sN
independent of the number of sites. In the raster vers
simply "move" a processor from site to site. Upon arr
a site, this processor must first load the local neighb
relations and state values, perform the replacement, ani
on. The time required to refresh S grows linearly with P
Thus, for example, for the purposes at hand, the paral
cedure is potentially at least 104 times faster than th4
version we used, and which required considerable CPU t
a VAX 780. Of course, we recognize that the fully
version will require extremely sophisticated new hal
although we understand that small prototypes of simi
chines are underway at several places.
A more modest degree of parallelism can be simply
mented. Since the convergence theorems are indepen
the details of the site replacement scheme nl, n2,
graph associated with the MRF X can be divided into
tions of sites with each collection assigned to an indepei
running (asynchronous) processor. Each such processoi
execute a raster scan updating of its assigned sites. Coi
cation requirements will be small if the division of th
respects the natural topology of the scene, provided, of
that the neighborhood systems are reasonably local. S
implementation, with five or ten micro- or minicompute
resents a straightforward application of available techno
XII. GIBBS SAMPLER: MATHEMATICAL FOUNDATI
As in Section XI, (nt), t = 1, 2, *.* ,is the sequence ir
the sites are visited for updating, and X,(t) denotes ti
of site s after t replacement opportunities, of which onl
for which n, = s, 1 < r < t, involve site s. For simplic
will assume a common state space As A = {0, 1, *,
and as usual that 0 < 7r(co) < 1 for all co E Q or, wha
same, that sup., U(.o)! < oo. The initial configuration i
We now investigate the statistical properties of the r
process {X(t), t = 0, 1, 2, . }. The evolution X(t - 1)
of the system was explained in Section XI. In mathe.
terms,
-

-

-

P(Xs(t) = Xss ES)
=

1(Xnt XntjXs xs, s :7nt)P(Xs(t- 1)
=

=

(12.1)
xS, s f nt)
where, of course, tr = eUlTIZ is the Gibbs measure which
=

drives the process. Our first result states that the distribution
of X(t) converges to 7r as t -* oo regardless of X(0). The only

731

assumption is that we continue to visit every site, obviously a
necessary condition for convergence.
Theorem A (Relaxation): Assume that for each s E S, the sequence {ft, t > 1 } contains s infinitely often. Then for every
E 2 and every co E Q,
starting configuration rC

lim P(X(t) =

t

o

coIX(O) = 71) = rr(o).

(12.2)

The proof appears in the Appendix, along with that of Theorem B. Like the Metropolis algorithm, the Gibbs Sampler produces a Markov chain {X(t), t = 0, 1, 2,
I with 7T as equilibrium distribution. The only complication is that the transition
probabilities associated with the Gibbs Sampler are nonstationary, and their matrix representations do not commute. This
precludes the usual algebraic treatment. These issues are discussed in more detail at the beginning of the Appendix.
We now turn to annealing. Hitherto the temperature has
been fixed. Theorem B is an "annealing schedule" or rate of
temperature decrease which forces the system into the lowest
energy states. The necessary programming modification in the
relaxation process is trivial, and the local nature of the calculations is preserved.
Let us indicate the dependence of ir on T by writing 7rT, and
let T(t) denote the temperature at stage t. The annealing procedure generates a different process {X(t), t = 1, 2, * * * } such
that
.

.

.

P(Xs(t) = x, s GES)
=

7rT(t)(Xnt = Xnt|Xs = xs, s nt)
=

P(X,(t 1) x, s =A nt).

(12.3)

Q0 {= E Q2: U(c) = min U(71)},

(12.4)

-

=

Let

and let ir0 be the uniform distribution on f20. Finally, define

U* = max U(co),

U* = min U(co),
Co

A=U*- U*.

(12.5)

Theorem B (Annealing): Assume that there exists an integer

r > N such that for every t = 0, 1, 2, - - - we have

S C {nt+1,nt+2,

,

,nt+,rl-

Let T(t) be any decreasing sequence of temperatures for which

a) T(t) -0 as t -*0;
b) T(t) > NA/log t
for all t > to for some integer to > 2.
Then for any starting configuration 7r E& Q and for every

cu E E2,

lim P(X(t) =

ljx(0) = 71) = 7ro(cL))

(12.6)

The first condition is that the individual "clocks" do not
slow to an arbitrarily low frequency as the system evolves, and
imposes no limitations in practice. For raster replacement,

732

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

= N. The major practical weakness is b); we cannot truly fol- all MAP estimates generated by the serial Gibbs Sampler with
low the "schedule" NA/log t. For example, with N = 20,000 annealing schedule
and A = 1, it would take e401000 site visits to reach T- 0.5.
C
We single out this temperature because we have obtained good
+ k) '
(
log
(
results by making T decrease from approximately T = 4 to T =
0.5 over 300-1000 sweeps (= 300N- 1OON replacements), where T(k) is the temperature during the kth iteration (= full
using a schedule of the form Cllog (1 + k), where k is the num- sweep of S), so that K is the total number of iterations. In
ber of full sweeps. (Notice that the condition in b) is then sat- each case, C = 3.0 or C = 4.0. No pre- or postfiltering, nor
isfied provided C is sufficiently large.) Apparently, the bound anything else was done. The models for the intensity and line
in b) is far from optimal, at least as concerns the constant NA. processes were kept as simple as possible; indeed, only cliques
(In fact, the proof of Theorem B does establish something of size two appear in the intensity model.
stronger, namely that A can be taken as the largest absolute
Group 1: The original image [Fig. 2(a)] is a sample of an
difference in energies associated with pairs w and w* which MRF on Z128 with L = 5 intensities and the eight-neighbor sysdiffer at only one coordinate. But this improvement still tem (Fig. 1, c = 2). The potentials Vc = 0 unless C = {r, s}, in
leaves NA too large for actual practice.) On the other hand, which case
the logarithmic rate is not too surprising in view of the widefs =fr
spread experience of chemists that T must be lowered very
V,3
Vc(f){=
slowly, particularly near the freezing point. Otherwise one en1, fs frcounters undesirable physical embodiments of local energy
Two hundred iterations (at T= 1) were made to generate
minima.
Concerning ergodicity, in statistical physics one attempts to Fig. 2(a).
predict the observable quantities of a system in equilibrium;
The first degraded version is Fig. 2(b), which is simply Fig.
the
"time
of
these are
averages" functions on Q2. Under the 2(a) plus Gaussian noise with a = 1.5 relative to gray levels f,
"ergodic hypothesis," one assuumes that (10.1) is in force, so 1 <f < 5. Fig. 2(c) is the restoration of Fig. 2(b) with K = 25
that time averages approach the corresponding "phase aver- iterations only, i.e., early in the annealing process. In Fig.
ages" or expected values. The analog for our system is the 2(d), K = 300.
The second degraded image [Fig. 3(b)] uses the model
assertion that, in some suitable sense,
I n
i-=H(F) 1/2 .S(31
lim Y(X(t))=
Y(o)drr(c).
(12.7)
--oO n
where , = 1 and a = 0.1, again relative to intensities 1 <f < 5.
(Here again T is fixed.) As we have already stated, a direct cal- Fig. 3(c) and 3(d) shows the restorations of Fig. 3(b) with K
25 and K 300, respectively.
culation of the righthand side of (12.7), namely,
Group 2: Fig. 4(a) is "hand-drawn." The lattice size is 64 X
64 and there are three gray levels. Gaussian noise (p = 0, a =
Y(WC)eU(W)/T eU(w)/T
0.7) was added to produce Fig. 4(b). We tried two types of
on Fig. 4(b). First, we used the "blob process"
restoration
is impossible in general. The left-hand side of (12.7) suggests
which
generated
Fig. 2(a) for the F -model. There was no line
that we use the Gibbs Sampler and compute a time average of
the function Y. For most physical systems, the ergodic hy- process and K = 1000. Obviously these are flaws; see Fig. 4(c).
A line process L was then adjoined to F for the original
pothesis is just that-a hypothesis-which can rarely be verified
image
model, and the corresponding restoration after 1000
in practice. Fortunately, for our system it is not too difficult
iterations
is shown in Fig. 4(d). L itself was described in Case
to directly establish ergodicity.
of
III and the neighborhood system for (F, L) on
2
Section
Theorem C (Ergodicity): Assume that there exists a such
U
was
discussed in Case 3 of Section 1II. The (prior)
Z64
D64
C
that S {fnt
* nt } for all t. Then for every function
\ = (F, L) was as follows. The range of F is
on
distribution
Y on 2 and for every starting configuration q C Q, (12.7)
=
3
{0,
1,
2}
(L
intensities). The energy U(f, 1) consists of
holds with probability one.
+ U(l). To understand the interaction
two terms, say
XIII. EXPERIMENTAL RESULTS
term
1), let d denote a line site, say between pixels r and
There are three groups of pictures. Each contains an original s. If Ld = 1, i.e., an edge element is "present" at d, then the
image, several degraded versions, and the corresponding resto- bond between s and r is "broken" and we set V{r, s}(fr' fs) =
rations, usually at two stages of the annealing process to illus- 0 regardless of f, fs; otherwise (Ld = 0) V{r, s} is as before extrate its evolution. The degradations are formed from com- cept that + X are replaced by +1. As for U(l), only cliques of
binations of
size four are nonzero, of which there are six distinct types up
to
rotations. These are shown in Fig. 5(a) with their assoi) 0 absent or 0(x)= -;
ciated
energy values.
ii) multiplicative or additive noise;
Then
we corrupted the hand-drawn figure using (13.1) with
iii) signal-to-noise levels.
the same noise parameters as Fig. 3(b), obtaining Fig. 6(b),
The signal-to-noise ratios are all very low. For blurring, we which is restored in Fig. 6(c) using the same prior on (1', L)
always took the convolution H in (2.3). The restorations are as above and with K = 1000 iterations.
=

L

=

r

+,

,

U(fI

U(fIl)

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

(a)

(c)

(b)

(d)

733

Fig. 2. (a) Original image: Sample from MRF. (b) Degraded image: Additive noise. (c) Restoration: 25 iterations. (d)
Restoration: 300 iterations.

Group 3: The results in Group 2 suggest a boundary-finding
algorithm for general shapes: allow the line process more
directional freedom. Group 3 is an exercise in boundary finding at essentially 0 dB. Fig. 7(a) is a 64 X 64 segment of a
roadside photograph that we obtained from the Visions Research Group at the University of Massachusetts. The levels
are scaled so that the (existing) two peaks in the histogram
occur at f = 0 and f = 1. We regard Fig. 7(a) as the blurred
image H( F). Noise is added in Fig. 7(b); the standard error is
a = 0.5 relative to the two main gray levels f = 0, 1.
Figs. 7(c) and 7(d) are "restorations" of Fig. 7(b) for K =
100 and K = 1000 iterations, respectively. The outcome of
the line process is indicated by painting black any pixels to the
left of or above a "broken bond." The two main regions, comprising the sign and the arrow, are perfectly circumscribed by a
continuous sequence of line elements.
The model for vX is more complex than the one in Group 2.
There are now four possible states for each line site corresponding to "off" (I = 0) and three directions, shown in Fig.
5(b). The U(f term is the same as before in that the pixel
bond between r and s is broken whenever Id * 0. The range of
F is{0,1}(L=2).

11)

Only cliques of size four are nonzero in U(I), as before.
However, there are now many combinations for (ldl, d2 I ,
Id4) given such a clique C {d,, d2, d3, d4 } of line sites, although the number is substantially reduced by assuming rotational invariance, which we do. Fig. 5(c) shows the convention we will use for the ordering and an example of the notation. The energies for the possible configurations (Idi, 1 6 i 6
4) range from 0 to 2.70. (Remember that high energies correspond to low probability, and that the exponential exaggerates differences.) We took V(0, 0, 0, 0) = 0 and V(ldi, I 6 i 6
4) = 2.70 otherwise, except when exactly two of the Id, are
nonzero. Parallel segments [e.g., (1, 0, 1, 0)] receive energy
2.70; sharp turns [e.g., (0, 2, 1, 0)] and other "corner" types
get 1.80; mild turns [e.g., (0, 2, 3, 0)] are 1.35; and continuations [e.g., (2, 0, 2, 0) or (0, 1, 3, 0)] are 0.90.
XIV. CONCLUDING REMARKS

We have introduced some new theoretical and processing
methods for image restoration. The models and estimates are
noncausal and nonlinear, and do not represent extensions into
two dimensions of one-dimensional filtering and smoothing

734

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

(a)

(C)

(b)

(d)

Fig. 3. (a) Original image: Sample from MRF. (b) Degraded image: Blur, nonlinear transformation, multiplicative noise.
(c) Restoration: 25 iterations. (d) Restoration: 300 iterations.

algorithms. Rather, our work is largely inspired by the methods of statistical physics for investigating the time-evolution
and equilibrium behavior of large, lattice-based systems.
There are, of course, many well-known and remarkable features of these massive, homogeneous physical systems. Among
these is the evolution to minimal energy states, regardless of
initial conditions. In our work posterior (Gibbs) distribution
represents an imaginary physical system whose lowest energy
states are exactly the MAP estimates of the original image
given the degraded "data."
The approach is very flexible. The MRF-Gibbs class of
models is tailor-made for representing the dependencies among
the intensity levels of nearby pixels as well as for augmenting
the usual, pixel-based process by other, unobservable attribute
processes, such as our "line process," in order to bring exogenous information into the model. Moreover, the degradation
model is almost unrestricted; in particular, we allow for deformations due to the image formation and recording processes.
All that is required is that the posterior distribution have a
"reasonable" neighborhood structure as a MRF, for in that
case the computational load can be accommodated by appro-

priate variants (such as the Gibbs Sampler) of relaxation algorithms for dynamical systems.
APPENDIX
PROOFS OF THEOREMS
Background and Notation
Recall that A= {0, 1,2,--- ,L - I is the common state
space, that ij, 77', c, etc. denote elements of the configuration
space Q2 = e, and that the sites S-{s= , s2, * * *, sN} are visited for updating in the order {n I, n2, * * } C S. The resulting stochastic process is {X(t), t = 0, 1, 2, * * 1, where X(0) is
the initial configuration.
For Theorem A, the transitions are governed by the Gibbs
distribution ir(co) = e U(,)ITIZ in accordance with (12.1),
whereas, for Theorem B (annealing), we use 7rT(t) (see Section
XII) for the transition X(t - 1) - X(t) [see (12.3)].
Let us briefly discuss the process {X(t), t> 0}, restricting
attention to constant temperature; the annealing case is essentially the same. To begin with, {X(t), t > O} is indeed a Markov chain; this is apparent from its construction. Fix t and
-

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

(a)

(c)

(b)

(d)

Fig. 4. (a) Original imnage: "Hand-drawn." (b) Degraded image: Additive noise. (c) Restoration: Without line process;
1000 iterations. (d) Restoration: Including line process; 1000 iterations.

o

o

0

0

o o0

O

0

o

o

0

O

0

0

0

o

O

0

0

0

oVo

(no lines)
V=O

(ending)
V=2.7

0

(turn)
V= 1.8

(continuation)
V =0.9

V= 1.8

V-2.7

(a)
I

O%0

060

e= 2
(b)
r
o

d4

d,

o

0

Q= 3

0

0

d2

o d3 0

0

0

( 0,1 ,0,2)
(c)
Fig. 5.

I
1o
0

0

o0o

e=

00

735

736

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

(a)

(b)

(c)

Fig. 6. (a) Original image: "Hand-drawn." (b) Degraded image: Blur,
nonlnear transformation, multiplicative noise. (c) Restoration: including line process; 1000 iterations.

X E Q2. For any x E A, let xx denote the configuration which
is x at site nt and agrees with X elsewhere. The transition matrix at time t is

X, = x3, s ¢ nt)

7T(Xnt = Xnt

(Md).q, w =

ir(W) = (7rMt). = E P(X(t) =

OrMt).,,

=

E

-1

otherwise

ir(n)(Md)1, .

xEA*0)MtW,

where (Mt),,,,,, denotes the row 7, column X entry of Mt, and
C= (XSI, XS2, * *, XsN). In particular, the chain is nonstationary, although clearly aperiodic and irreducible (since 7r(co) >
0 V X). Moreover, given any starting vector (distribution) ,uO,
the distribution of X(t) is given by the vector ,uo nt= l M1, i.e.,

=

(Mt)z'X"^IE A)

=7r(Xnl=Xnt=

t

mi
WO = ) = oxX fM
Pi,((t

=EP(X) = X(o)
I =

(A.l)

To see this, fix t and co = {x,}, and write

if 71 = coX for some x EA

O,

WI X(O) = 71) w(n).

12

=X,, S

Xs

(for any x' EA)

*nt) ir(X,

=x,

s *lnt)

= ir(Co).

Uto

(n).

Notice that ir is the (necessarily) unique invariant vector, i.e.,
for every t = 1, 2, * *,

It will be convenient to use the following, semistandard notation for transitions. For nonnegative integers r < t and
co,

E 2,

P(t,

set

coIr, 7) P(X(t) coIX(r) r)
=

=

=

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

(a)

(c)

(b)

(d)

737

Fig. 7. (a) Blurred image (roadside scene). (b) Degraded image: Additive noise. (c) Restoration including line process; 100 iterations. (d)
Restoration including line process; 1000 iterations.

and, for any distribution p on Q2, set

Obviously K(t) -

-

at

t -

-.

The

proof of Theorem A is

based on the following lemma, which also figures in the proof
P(t, r, ,u) =E P(t, r, ) u(72).
of the annealing theorem.
Lemma 1: There exists a constant r, 0 < r < 1, such that for
Finally, ||,u - v denotes the L'1 distance between two distri- every t = I, 2, *,

I

coI

-

butions on Q2:

SUp

II1 - vll = E 1p(C@) - v(CO)I.
Obviously, u,, -+,(n
Vco) if and only if iI
is finite.)

I1

cc)

in distribution (i.e., ,un(C)

=OIX(°)
-

J(w)

,IIU *0, n cc. (Remember that Q2

k= 1,2,

This is possible since every site is visited infinitely often.
Clearly (at least) k iterations or full sweeps have been completed by "time" Tk. In particular, kN < Tk < - V k. Let
K(t) = sup {k: Tk < t}.

rK(t)

sup |P(X(t)

lim

,nTk},

'")I <

Assume for now that the lemma is true. Since nr is an invariant vector for the chain:

Proof of Theorem A: Set To = 0 and define T1 < T2 < -.

such that

SC{nTk l+,nTk-l+22,

|PX(t) = cWX(0) = ') - PX(t)

=CUIx(o) =n
=

lim

sup

=W

()

| ir(r'){P(X(t)

= XIx(o) = 71) - P(Xt) = w I xP = nT)}

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

738

[by (A.1)]
< rim
-

|P(

sup

Certainly, for each c C Q,

,,IX(O) ')

=

sup E P(X(t) = oj X(TI ) @ P(X(TI )c- '| X(O) 77)

=

=

P(X(t) = WI X(O) = W")

< sup
psup E P(X(t) = coIX(TI ) o) p(c )

O, by Lemma 1.

l

So it suffices to prove Lemma 1.
Proof of Lemma 1. For each k = 1, 2, * and 1 < i < N,
let mi be the time of the last replacement of site si before Tk +

1,<i.e.,

mi = sup {t: t Tk, nt =Si}-

We can assume, without loss of generality, that mI >m2 >
**> mN; otherwise, relabel the sites. For any = (xs1,**,
xSN) and co',

where the supremum is over all probability measures p on Q
which, by (A.2), are subject to p(Co')> N V Co'. Suppose
co) --P(X(t) = X(TI) = ') is maximized at W' - c* (which
depends on w). Then the last supremum is attained by placing mass 6N on each co' and the remaining mass, namely, 1 I
N = 1 -LN6N, on co*. The value so obtained is

coI

(I1

P(X(Tk)>=WX(Tk

Similarly,

=

XsN IX(Tk-l)

co

inf L P(X(t) =c X(TI)=co') P(X(TI)=co' X(O)=1)

)

77

WI

> (I_ (LN

N

P(Xs.(mi) xs. XSi +r(mi+I)

=

Xsj + 1

,

XsN(mN) = Xs,, X(Tk - 1)

(XS,,

inf
,

xSN) E n

7F(Xs,

=

xjsil

Xs. = XSj, j

Then 0 < 6 < 1 and a little reflection shows that every term in
the product above is at least 6. Hence,
inf

P(X(Tk) = CIX(Tk_1)

)

(A.2)

o)

=

WII
17n

and hence,
SUp IP(X(t) = cIoX(0) = 7i') - P(X(t) = coIX(o) = '')
< (1 - L 6N)

=

sup {sup P(X(t) = wIX(0) = 71)
- inf

=

X(o) = 77)}
=

X(Tj)

c')P(X(T1) = ,'IX(O)=

inf
=

co

P(X(t)

SUp

= sup

=

P(X(t) =

c')

P(X(t)

P(X(T1)

sup Q(t, co).
w

=

cIX(T,)

Co'|X(O)

=

sup
I

(4, 17, 17

,,

|P(X(t)

= co|X(To) = 7 ) - P(X(t)

IX(0)=q') P(X(t)=- IX(O)=')

P(X(t) =

P(X(t) = wlX(T1) =co*)

Q(t, co) < (1 - LN6N) {P(X(t)
= co!X(Tl) = c*)- P(X(t)c= oX(Tl)c= *)},

Consider now the inequality asserted in Lemma 1. It is trivial for t < T, since in this case K(t) = 0. For t > T,
sup

S

where co'- P(X(t)= oIX(TI wJ) is minimized at cu*. It

Li4, W

(.4,

N) P(X(t)

follows immediately that

i).

I < i <N

k=1, 2,-"

+ 6N

)-

1)

IX(TI) =*)

-

Let 6 be the smallest probability among the local characteristics:
6=

P(X(t) coIX(T1 co).=@

+ 5N

w')

-)

Ax, (IT, ) Xs,--,XsN(MN)
=

(LN 1) 5N )P(X(t)
= WJX(T1) = w*)

co

CO

IX(TI )=,q) 1.

Proceeding in this way, we obtain the bound

(1 - LN6N)K(t) SUp |P(X(t)

= X(TK(t)) = r1') - P(X(t) = X(TK(t))

11)I

and the lemma now follows with r= - LN&N . Notice that
r = 0 corresponds to the (degenerate) case in which 6 = L1,
Q.E.D.
i.e., all the local characteristics are uniform on A.
Proof of Theorem B: We first state two lemmas.
Lemma 2: For every to = 0, 1, 2, * * ,
lim

sup

t -*o00
t w, 1, 17 1'

=

P(X(t)

IM

w IX(to) = 7') - P(X(t) = wIjX(to) = r")I = 0-

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

Lemma 3:

lim

to0-o

sup

t) to

739

Now fix k for the moment and define the mi as before:

IIP(t, Jto, ro)- 7roj=O.

mi=sup {t: t. Tk,nt =si},

1 <iS<N.

We again assume that m 1 > m2 > > mN. Then
Recall that rr0 is the uniform probability measure over the
P(X(Tk) = c X(Tk - 1) =co ')
minimal energy states Q20 = {co: U(co) = min, U(t1)}.
First we show how these lemmas imply Theorem B, which
=
P(X,, (m 1 ) = X.-, ,XsN$(mN)
states that P(X(t) = X(O) = 7) converges to 7r0 as t oo. For
=XSNIX(Tk-l) )
any7GEfl,
N
lim ||P(X(t)
X(O) =7) - iro|
= fl P(XS.(mj)=
1 (mj +)
t
...

I

=

xsJIXsj+

00

j=1

lim 12P(t,. to,1')

= him

XsN(MN) = XsN, X(Tk 1 )=

to ° tt+ 00

=Xsy

*P(to, n'O,
0 )- roJ'

> H

(m1) (using (12.3) and the definition of l)

> L -N

nl CAIT(mi)

t> to

< lim

N

j=1

P(t,- Ito,7t)

in
lim

to-+00 t-*00
t) to

7I

)

N

j=l

*P(to,r' 0,
O,)-P(t,I to,rro)JJ
+ lim

-

T(to +

lim ||P(t, - Ito 7)IO)- .Toll

= to + kr, j = 1, 2,

to +00 to+00

t > to

-

kr)

(since mi < Tk

*-, N, and T( - ) is decreasing)

LN(to +kr)'
The last term is zero by Lemma 3. Furthermore, since P(to,wherever to + kr is sufficiently large. In fact, for a sufficiently
Jo, 7) and 70 have total mass 1, we have
small constant C, we can and do assume that
P(t,* I to ,71iP(to ,77' 0, q)-P(t to,ITO1O)l
> CL-N
J~~k1
inf P(X(Tk)=W|X(Tk-l)=
)>
(A.3)
to + kr
in,
W'
= Z SUP 1 (P(t, CO to, ')-P(t, fI to, "1))
C077
71
for every to = 0, 1, 2, - * * and k = 1, 2,*, bearing in mind
X (P(to, n °, 71)-ro(1)
that Tk depends on to.
For each t > to, define K(t) = sup {k: Tk < t} so that K(t) <2 E sup
oo as t - oo. Fix t > T1 and continue to follow the argument
w n, n.,
in Lemma 1, but using (A.3) in place of (A.2), obtaining
Finally, then,
sup P(X(t) =
- P(X(t) =
lim ||P(X(t) = X(O) = 77)- rTo
CO, 7, r1
t
WI~~~
'h'(i
K(t) t
tC
Z
lim
<2
lim
sup

'l
JP(t,&l to,n7)-P(t,&4to,71")J.

coX(to)

JX(to) =7)

-00

to

- P(t,

I

> to
tt.°

JP(tcj)Ito17?')

t

coI to, q7)l

Hence it will be sufficient to show that

Q.E.D.
m
by Lemma 2.
c
lim H 1
=0
(A.4)
M
Proof of Lemma 2: We follow the proof of Lemma 1. Fix
to + kr
k 1
to = 0, 1,-" and define Tk = to + k-r, k = 0, 1, 2,-- -. Re- for every to. However, (A.4) is a well-known consequence of
call that S C {nt+t,... ,nt +} for all t by hypothesis, that
divergence of the series 2k (to + kr)-' for all to, -r. This
lrT(t)(W)= eU(w))/T(t)IZ and that U*, U* are the maximum the
the proof of Lemma 2.
and minimum of U(c), respectively, the range being A = U* - completes
Lemma 3: The probability measures P(t, to, nro)
Proof
of
U*. Let
figure prominently in the proof, and for notational ease we
inf
prefer to write Pto t( ), sSo that for any t > to > 0 we have
1TT(t)(Xsi Xsi xs; XSj, j
1 < iSN
(Xsl-. ..XSN ) E
Pto, t(c) = E P(X(t) = co X(to) =17) iro(74).
=0

--* 00

=

Observe that

e-U*/T(t)

6() L-U(t)

t =

I)Tt
e

I()

=

=

i).

To begin with, we claim that for any t > to > 0,

| |Pto, t - 7T(t)|| < | Pto, t.1 - 1TT(t)||-

(A.5)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. PAMI-6, NO. 6, NOVEMBER 1984

740

Assume for convenience that nt = si. Then

|Pto, t

Lemma 3 can now be obtained from (A.5) and (A.7) in the
following way. Fix t > to > 0:

T(tM
-Z
(XS, *

XSN)

17TT(t)(Xsl = XS1 Xs= X s

S1)

I PPtot+Troll

S I tO,t- 7T(t)I + 117T(t) iTohI
o

*Pto , t - I (Xs = XS,s * sS1)

- 1tT, t

2:
|
E:
T(t)(Xsl =Xsllxs=xs,s*sl)
XS2,9..,XSN XSl A

tZ Pto , t-(X5x5=sXs

<
it)

I E {Pto't-I(xs=xS'sES)

=

IPt,, t 1 (Xs = x s E S)
C-EQ
-

(Xslt* *X x=SN)
-'7rT(t)(Xs = xs, S E

Observe that ?Io - 7rT(t)l -*0 as t -* oo. To see this, let
o be the size of QO. Then
e -U(iJ )/T(t)

IO

:T= e -U(w)/T(t) + Lj'E=- M o -U(T')(T(t)

E

12ol+

e

e -(U(w)

U*)/T(t)
e- (U(Lj') uic )/ T(t)

0,
0
t °°

>

I

(u) C QO .

Next, we claim that

7IrT(t) -

rT(t + 1)

11 rT(t)

ITT(t + 1

< -(A.7)

Since
00

t=1

t-1

7TT(to)II + E|iITT(k)
k= to

7TT(k+ O)II + |I|?T(t) - 'No011

Since Pto, to = iTo and || 7T(t) t

sup llPto,t - gToll
sup

lim

Z

t-I

£

to°°+ t>to k=to

to-°O k=to

=0

-+

O as t-+ oo, we have,

to

< lim
=

iroI

I |11T(k) iTT(k + 1) II

1|7TT(k)- IT(k+1)1J

due to (A.7).

Q.E.D.

ACKNOWLEDGMENT

The authors would like to acknowledge their debt to U.
Grenander for a flow of ideas; his work on pattern theory
[23] prefigures much of what is here. They also thank D. E.
McClure and S. Epstein for their sound advice and technical
assistance, and V. Mirelli for introducing them to the practical
side of image processing as well as arguing for MRF scene
models.
REFERENCES

00

£

-

lim

w' EQ- o

-

Proceeding in this way,

to -*

= l!Pto,t-i - 7TT(t)ll.

T(t)(W)+==

7rT(t I

Il7lT(t) -T11o.

IIPto,t- 7To11I IPto,to

7rT(t)(Xs XS, S c S)l

L

by (A.5)

1lPto,t-2 - 7rT(t-)2) + IrT(t-2)
7TT(t i)11 + j7|1T(t -) 7TT(t)|
+

XS2 -*XSN xs1
<

+

To1,

-

1TT(t)(Xs = Xs, S * SI)

= E

7T(t - )

7rT(t)II+ ||aT
I ) TOI

=A S1 )
7TT(to tl(Xs
- T(t)(x=Xss S#1) sI
... XS

70

< |Pto, t -2

= Xs, S

-

I1 T(t)

< I1Pto,t-1 - 7T(t-l)|l + 117T(t-l)
- 71T(t)11 + 1 | 7T(t) - N1o1

-TT(t)(Xs =Xs,S SE s)

XS

-

co

ZE |7T(t)(CO)

X t=1

7rT(t + 1)(CU)|

and since 7rT(t)(CO) _* iro (C) for every co, it will be enough to
show that, for every co, 7TTCO) is monotone

(increasing or de-

creasing) in T for all T sufficiently small. But this is clear from
(A.6): if co 1 Q20, then a little calculus shows that 7rT(W) iS
strictly increasing for Te (0, e) for some c, whereas if CUG
Q2O, then 7TT(CO) is strictly decreasing for all T> 0.

[1] K. Abend, T. J. Harley, and L. N. Kanal, "Classification of binary
random patterns," IEEE Trans. Inform. Theory, vol. IT-Il, pp.
538-544, 1965.
[2] H. C. Andrews and B. R. Hunt, DigitalImage Restoration. Englewood Cliffs, NJ, Prentice-Hall, 1977.
[3J M. S. Bartlett, The Statistical Analysis of Spatial Pattern. London: Chapman and Hall, 1976.
[4] T. Berger and F. Bonomi, "Parallel updating of certain Markov
random fields," preprint.
[51 J. Besag, "Nearest-neighbor systems and the auto-logistic model
for binary data," J. Royal Statist. Soc., series B, vol. 34, pp. 7583, 1972.
, "Spatial interaction and the statistical analysis of lattice sys[61
tems (with discussion)," J. Royal Statist. Soc., series B, vol. 36,
pp. 192-326, 1974.
[7] A. B. Bortz, M. H. Kalos, and J. L. Lebowitz, "A new algorithm

GEMAN AND GEMAN: STOCHASTIC RELAXATION, GIBBS DISTRIBUTIONS, AND BAYESIAN RESTORATION

for Monte Carlo simulation of Ising spin systems," J. Comp.
Phys., vol. 17, pp. 10-18, 1975.
(8] V. Cerny, "A thermodynamical approach to the traveling salesman problem: an efficient simulation algorithm," preprint, Inst.
Phys. & Biophys., Comenius Univ., Bratislava, 1982.
[91 P. Cheeseman, "A method of computing maximum entropy probability values for expert systems," preprint.
[10] R. Chellappa and R. L. Kashyap, "Digital image restoration using
spatial interaction models," IEEE Trans. Acoust., Speech, Signal
Processing, voL ASSP-30, pp. 461-472, 1982.
[11] D. B. Cooper and F. P. Sung, "Multiple-window parallel adaptive
boundary finding in computer vision," IEEE Trans. Pattern Anal.
Machine Intell., vol. PAMI-5, pp. 299-316, 1983.
[121 G. C. Cross and A. K. Jain, "Markov random field texture models," IEEE Trans. Pattern AnaL Machine Intell., vol. PAMI-5,
pp. 25-39, 1983.
[13] L. S. Davis and A. Rosenfeld, "Cooperating processes for lowlevel vision: A survey," 1980.
[14] H. Derin, H. Elliott, R. Christi, and D. Geman, "Bayes smoothing
algorithms for segmentation of images modelled by Markov random fields," Univ. Massachusetts Tech. Rep., Aug. 1983.
[15] R. L. Dobrushin, "The description of a random field by means
of conditional probabilities and conditions of its regularity,"
Theory Prob. Appl., vol. 13, pp. 197-224, 1968.
[16] H. Eliott, H. Derin, R. Christi, and D. Geman, "Application of
the Gibbs distribution to image segmentation," Univ. Massachusetts Tech. Rep., Aug. 1983.
[17] H. EDiott, F. R. Hansen, L. Srinivasan, and M. F. Tenorio, "Application of MAP estimation techniques to image segmentation,"
Univ. Massachusetts Tech. Rep., 1982.
[18] P. A. Flinn, "Monte Carlo calculation of phase separation in a
2-dimensional Ising system," J. Statist. Phys., vol. 10, pp. 89-97,
1974.
[19] B. R. Frieden, "Restoring with maximum likelihood and maximum entropy,"J. Opt. Soc. Amer.,vol. 62,pp. 511-518, 1972.
[20] D. Geman and S. Geman, "Parameter estimation for some Markov random fields," Brown Univ. Tech. Rep., Aug. 1983.
[21] S. Geman, "Stochastic relaxation methods for image restoration
and expert systems," in Proc. ARO Workshop: Unsupervised
Image Analysis, Brown Univ., 1983; to appear in Automated
Image Analysis: Theory and Experiments, D. B. Cooper, R. L.
Launer, and D. E. McClure, Eds. New York: Academic, 1984.
[22] U. Grenander, Lectures in Pattern Theory, Vols. I-Ill. New
York: Springer-Verlag, 1981.
[23] D. Griffeath, "Introduction to random fields," in Denumerable
Markov Chains, Kemeny, Knapp and Snell, Eds. New York:
Springer-Verlag, 1976.
[24] A. Habibi, "Two-dimensional Bayesian estimate of images,"
Proc. IEEE, vol. 60, pp. 878-883, 1972.
[25] F. R. Hansen and H. Elliott, "Image segmentation using simple
Markov field models," Comput. Graphics Image Processing, vol.
20, pp. 101-132, 1982.
[26] A. R. Hanson and E. M. Riseman, "Segmentation of natural
scenes," in Computer Visions Systems. New York: Academic,
1978.
[27] J. M. Hammersley and D. C. Handscomb, Monte Carlo Methods.
London: Methuen, 1964.
[28] M. Hassner and J. Sklansky, "The use of Markov random fields as
models of texture," Comput. Graphics ImageProcessing, vol. 12,
pp. 357-370, 1980.
[29] G. E. Hinton and T. J. Sejnowski, "Optimal perceptual inference," in Proc. IEEE Conf Comput. Vision Pattern Recognition,
1983.
[30] R. A. Hummel and S. W. Zucker, "On the foundations of relaxation labeling processes," IEEE Trans. Pattern Anal. Machine Intell., vol. PAMI-5, pp. 267-287, 1983.
[31] B. R. Hunt, "Bayesian methods in nonlinear digital image restoration," IEEE Trans. Comput., vol. C-23, pp. 219-229, 1977.

741

[32] V. Isham, "An introduction to spatial point processes and Markov random fields," Int. Statist. Rev., vol. 49, pp. 21-43, 1981.
[331 E. Ising, Zeitschrift Physik, vol. 31, p. 253, 1925.
[34] A. K. Jain, "Advances in mathematical models for image processing,"Proc. IEEE, vol. 69, pp. 502-528, 1981.
[351 A. K. Jain and E. Angel, "Image restoration, modeling and reduction of dimensionality," IEEE Trans. Comput., vol. C-23, pp.

470-476, 1974.
[36] E. T. Jaynes, "Prior probabilities," IEEE Trans. Syst. Scf Cybern., vol. SSC-4, pp. 227-241, 1968.
[37] L. N. Kanal, "Markov mesh models," in Image Modeling. New
York: Academic, 1980.
[381 R. L. Kashyap and R. Chellappa, "Estimation and choice of
neighbors in spatial interaction models of images," IEEE Trans.

Inform. Theory, vol. IT-29, pp. 60-72, 1983.
[39] R. Kinderman and J. L. Snell, Markov Random Fields and Their
Applications. Providence, RI: Amer. Math. Soc., 1980.
[40] S. Kirkpatrick, C. D. Gellatt, Jr., and M. P. Vecchi, "Optimization by simulated annealing," IBM Thomas J. Watson Research
Center, Yorktown Heights, NY, 1982.
[41] P. A. Levy, "A special problem of Brownian motion and a general theory of Gaussian random functions," in Proc. 3rd Berkeley
Symp. Math. Statist. and Prob., vol. 2, 1956.
142] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller,
and E. Teller, "Equations of state calculations by fast computing
machines,"J. Chem. Phys., vol. 21, pp. 1087-1091, 1953.
[43] N. E. Nahi and T. Assefi, "Bayesian recursive image estimation,"
IEEE Trans. Comput., vol. C-21, pp. 734-738, 1972.
[441 D. K. Pickard, "A curious binary lattice process," J. Appl. Prob.,
vol. 14, pp. 717-731, 1977.
[45] W. H. Richardson, "Bayesian-based iterative method of image
restoration," J. Opt. Soc. Amer., vol. 62, pp. 55-59, 1972.
[46] A. Rosenfeld, R. A. Hummel, and S. W. Zucker, "Scene labeling
by relaxation operations," IEEE Trans. Syst., Man, Cybern.,
vol. SMC-6, pp. 420-433,197.
[47] A. Rosenfeld and A. C. Kak, Digital Picture Processing, vols.
1, 2, 2nd ed. New York: Academic, 1982.
[48] F. Spitzer, "Markov random fields and Gibbs ensembles," Amer.
Math. Mon., vol. 78, pp. 142-154, 1971.
[49] J. A. Stuller and B. Kruz, "Two-dimensional Markov representations of sampled images," IEEE Trans. Commun., vol. COM-24,
pp. 1148-1152, 1976.
[50] H. J. Trussell, "The relationship between image restoration by
the maximum a posteriori method and a maximum entropy
method," IEEE Trans. Acoust., Speech, Signal Processing, vol.
ASSP-28, pp. 114-117, 1980.
[51] J. W. Woods, "Two-dimensional discrete Markovian fields,"
IEEE Trans, Inform. Theory, voL IT-18, pp. 232-240, 1972.
Stuart Geman received the B.A. degree in physics from the University of Michigan in 1971,
the M.S. degree in physiology from Dartmouth
College in 1973, and the Ph.D. degree in applied mathematics from the Massachusetts Institute of Technology in 1977.
Since 1977 he has been a member of the Division of Applied Mathematics at Brown University, Providence, RI, where he is currently an
Associate Professor. His research interests in-

clude statistical inference, parallel computing,
image processing, and stochastic processes.
Dr. Geman is an Associate Editor of The Annals of Statistics and is a
recipient of the Presidential Young Investigator Award.
Donald Geman, for a photograph and biography, see this issue, p. 720.

